{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import gensim\n",
    "import bz2\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/WEEK_2_DESCRIPTIONS.csv').drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_scores(score):\n",
    "    if score > 60:\n",
    "        if score > 75:\n",
    "            return 2\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "df['success'] = df['Critic_Score'].apply(bin_scores)\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    # Removes punctuation\n",
    "    words = [''.join(ch for ch in s if ch not in string.punctuation)\\\n",
    "             for s in text.split()]\n",
    "    \n",
    "    # Returns the lower-case string\n",
    "    return ' '.join(words).lower()\n",
    "\n",
    "df.plots = df.plots.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df['tokens'] = df.plots.apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = '../data/enwiki_20180420_win10_500d.txt.bz2'\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, limit=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_all_word2vec(tokens_list, vector, generate_missing=False, k=300, sent_length=100):\n",
    "    imputed = 0\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "        \n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    for _ in range(sent_length - len(tokens_list)):\n",
    "        vectorized.append(np.zeros(k))\n",
    "    return np.array(vectorized[:sent_length])\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_questions, generate_missing=False, k=300, sent_length=100):\n",
    "    embeddings = df['tokens'].apply(lambda x: get_all_word2vec(x, vectors, generate_missing=generate_missing, k=k, sent_length=sent_length))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_LENGTH = 50\n",
    "\n",
    "embeddings = get_word2vec_embeddings(word2vec, df, True, 500, SENTENCE_LENGTH)\n",
    "\n",
    "X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(embeddings, df.success, \n",
    "                                                                                        test_size=0.2, random_state=40)\n",
    "\n",
    "\n",
    "\n",
    "X_train_cnn =np.array(X_train_word2vec).reshape((-1, SENTENCE_LENGTH, 500, 1))\n",
    "X_test_cnn = np.array(X_test_word2vec).reshape((-1, SENTENCE_LENGTH, 500, 1))\n",
    "y_train_cnn = to_categorical(y_train_word2vec)\n",
    "y_test_cnn = to_categorical(y_test_word2vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_kum_cnn_graph(sent_len, word_vec, out_dim, filters = 64, n_grams = [2,3], num_dense_layers = 3):\n",
    "    '''\n",
    "    args:\n",
    "            sent_len: length of input sentense. if raw sentense is less than it, using zero padding, else cut down to it.\n",
    "        word_vec: dim of word vector embedding using pre-trained glove or word2vec model\n",
    "        out_dim: dim of output y\n",
    "        filters: filters for Convolutional layers\n",
    "        n_grams: list of ngram for Convolutional layers kernal. each will generate one cell output. details can be referred from paper\n",
    "        num_dense_layers: to decide how many dense layers after concatenating all Convolutional layers output\n",
    "    returns:\n",
    "        Keras Model\n",
    "    '''\n",
    "    inputs = keras.layers.Input(shape=(sent_len, word_vec, 1))\n",
    "    merged_layer = []\n",
    "    for h in n_grams:\n",
    "        conv_layer = keras.layers.Conv2D(filters, (h, word_vec), activation='relu')(inputs)\n",
    "        pool_layer = keras.layers.MaxPooling2D(pool_size=(sent_len-h+1, 1))(conv_layer)\n",
    "        merged_layer.append(pool_layer)\n",
    "    concat_layer = keras.layers.concatenate(merged_layer)\n",
    "    flatten_layer = keras.layers.Flatten()(concat_layer)\n",
    "    in_ = flatten_layer\n",
    "    prev_units = filters * len(n_grams)\n",
    "    for _ in range(num_dense_layers - 1):\n",
    "        prev_units /= 2\n",
    "        dense_layer = keras.layers.Dense(int(prev_units), \n",
    "                        activation='relu', \n",
    "                        kernel_regularizer = keras.regularizers.l2(0.01),\n",
    "                        # activity_regularizer = keras.regularizers.l1(0)\n",
    "                                        )(in_)\n",
    "        drop_layer = keras.layers.Dropout(.5)(dense_layer)\n",
    "        in_ = drop_layer\n",
    "        \n",
    "    outputs = keras.layers.Dense(out_dim, activation = 'softmax')(in_)\n",
    "    \n",
    "    model = keras.models.Model(inputs = inputs, outputs = outputs)\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4399 samples, validate on 1100 samples\n",
      "Epoch 1/20\n",
      "4399/4399 [==============================] - 11s 2ms/step - loss: 2.7026 - acc: 0.3705 - val_loss: 1.6973 - val_acc: 0.4127\n",
      "Epoch 2/20\n",
      "4399/4399 [==============================] - 9s 2ms/step - loss: 1.4661 - acc: 0.4083 - val_loss: 1.3121 - val_acc: 0.4636\n",
      "Epoch 3/20\n",
      "4399/4399 [==============================] - 10s 2ms/step - loss: 1.2364 - acc: 0.4496 - val_loss: 1.1734 - val_acc: 0.4755\n",
      "Epoch 4/20\n",
      "4399/4399 [==============================] - 9s 2ms/step - loss: 1.1281 - acc: 0.4828 - val_loss: 1.1261 - val_acc: 0.4627\n",
      "Epoch 5/20\n",
      "4399/4399 [==============================] - 9s 2ms/step - loss: 1.0382 - acc: 0.5297 - val_loss: 1.0130 - val_acc: 0.5418\n",
      "Epoch 6/20\n",
      "4399/4399 [==============================] - 9s 2ms/step - loss: 0.9310 - acc: 0.5954 - val_loss: 0.9756 - val_acc: 0.5555\n",
      "Epoch 7/20\n",
      "4399/4399 [==============================] - 9s 2ms/step - loss: 0.8173 - acc: 0.6686 - val_loss: 0.9349 - val_acc: 0.6036\n",
      "Epoch 8/20\n",
      "4399/4399 [==============================] - 9s 2ms/step - loss: 0.7177 - acc: 0.7340 - val_loss: 0.9496 - val_acc: 0.6200\n",
      "Epoch 9/20\n",
      "4399/4399 [==============================] - 9s 2ms/step - loss: 0.6600 - acc: 0.7702 - val_loss: 1.0148 - val_acc: 0.6209\n",
      "Epoch 10/20\n",
      "4399/4399 [==============================] - 9s 2ms/step - loss: 0.5903 - acc: 0.8170 - val_loss: 0.9728 - val_acc: 0.6491\n",
      "Epoch 11/20\n",
      "4399/4399 [==============================] - 9s 2ms/step - loss: 0.5243 - acc: 0.8509 - val_loss: 1.0394 - val_acc: 0.6427\n",
      "Epoch 12/20\n",
      "4399/4399 [==============================] - 9s 2ms/step - loss: 0.5004 - acc: 0.8611 - val_loss: 1.0673 - val_acc: 0.6273\n",
      "Epoch 13/20\n",
      "4399/4399 [==============================] - 9s 2ms/step - loss: 0.4704 - acc: 0.8750 - val_loss: 1.2343 - val_acc: 0.6282\n",
      "Epoch 14/20\n",
      "4399/4399 [==============================] - 9s 2ms/step - loss: 0.4581 - acc: 0.8752 - val_loss: 1.1494 - val_acc: 0.6518\n",
      "Epoch 15/20\n",
      "4399/4399 [==============================] - 9s 2ms/step - loss: 0.4285 - acc: 0.8938 - val_loss: 1.2266 - val_acc: 0.6073\n",
      "Epoch 16/20\n",
      "4399/4399 [==============================] - 9s 2ms/step - loss: 0.4067 - acc: 0.8993 - val_loss: 1.1972 - val_acc: 0.6273\n",
      "Epoch 17/20\n",
      "4399/4399 [==============================] - 9s 2ms/step - loss: 0.4017 - acc: 0.8963 - val_loss: 1.1647 - val_acc: 0.6355\n",
      "Epoch 18/20\n",
      "4399/4399 [==============================] - 9s 2ms/step - loss: 0.3706 - acc: 0.9118 - val_loss: 1.1825 - val_acc: 0.6427\n",
      "Epoch 19/20\n",
      "4399/4399 [==============================] - 9s 2ms/step - loss: 0.3567 - acc: 0.9150 - val_loss: 1.2255 - val_acc: 0.6564\n",
      "Epoch 20/20\n",
      "4399/4399 [==============================] - 9s 2ms/step - loss: 0.3398 - acc: 0.9216 - val_loss: 1.2992 - val_acc: 0.6300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x266b0b66f60>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_kum_cnn_graph(SENTENCE_LENGTH, 500, 3, n_grams=[1,2,3,4,5])\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "model.fit(X_train_cnn, y_train_cnn, epochs=20,\n",
    "          batch_size=100, validation_data=(X_test_cnn, y_test_cnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_kum_cnn_graph(100, 500, 3, n_grams=[2,3,4])\n",
    "\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#                   optimizer='adam',\n",
    "#                   metrics=['acc'])\n",
    "\n",
    "# model.fit(X_train_cnn, y_train_cnn, epochs=20,\n",
    "#           batch_size=40, validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
